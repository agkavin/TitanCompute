# TitanCompute

**Production-ready distributed LLM inference system with zero-proxy streaming architecture and intelligent MCDA scheduling.**

## ğŸ¯ What is TitanCompute?

TitanCompute is a high-performance distributed system for Large Language Model (LLM) inference that eliminates traditional bottlenecks through **zero-proxy streaming**. Instead of routing inference data through a central proxy, TitanCompute's coordinator intelligently selects optimal compute nodes and provides clients with direct streaming connections.

## âœ¨ Key Features

- **ğŸ§  Memory-Aware MCDA Scheduling**: Multi-criteria agent selection (40% VRAM + 30% Jobs + 20% RTT + 10% Performance)
- **ğŸ”„ Zero-Proxy Streaming**: Direct client-to-agent connections for maximum throughput
- **ğŸ”’ JWT Authentication**: RSA-256 signed tokens for secure agent access
- **âš–ï¸ GGUF Quantization**: Intelligent model quantization based on available system memory
- **ğŸ›¡ï¸ Circuit Breaker Fault Tolerance**: Automatic failure detection and recovery
- **ğŸ³ Production Ready**: Docker deployment with GPU support and health monitoring

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph "Client Layer"
        C[ğŸ–¥ï¸ Client Application<br/>REST/gRPC]
    end
    
    subgraph "Coordination Layer"
        CO[ğŸ§  Coordinator<br/>Go + gRPC<br/>Port: 50051]
        
        subgraph "MCDA Scheduling Engine"
            MCDA[ğŸ“Š Multi-Criteria Decision<br/>40% VRAM + 30% Jobs<br/>20% RTT + 10% Performance]
            JWT[ğŸ” JWT Token Generation<br/>RSA-256 Signed<br/>5min TTL]
        end
    end
    
    subgraph "Compute Layer"
        A1[ğŸ¤– Agent 1<br/>Python + Ollama<br/>Port: 50052]
        A2[ğŸ¤– Agent 2<br/>Python + Ollama<br/>Port: 50053]
        A3[ğŸ¤– Agent N<br/>Python + Ollama<br/>Port: 5005X]
    end
    
    subgraph "Model Layer"
        M1[ğŸ“¦ GGUF Models<br/>Auto-Quantized<br/>Q8_0 â†’ IQ2_M]
        M2[ğŸ“¦ GGUF Models<br/>Auto-Quantized<br/>Q8_0 â†’ IQ2_M]
        M3[ğŸ“¦ GGUF Models<br/>Auto-Quantized<br/>Q8_0 â†’ IQ2_M]
    end
    
    %% Request Flow
    C -->|1ï¸âƒ£ POST /inference/request| CO
    CO --> MCDA
    MCDA --> JWT
    JWT -->|2ï¸âƒ£ Optimal Agent + JWT| C
    
    %% Direct Streaming (Zero-Proxy)
    C -.->|3ï¸âƒ£ Direct gRPC Stream<br/>with JWT Auth| A1
    C -.->|3ï¸âƒ£ Direct gRPC Stream<br/>with JWT Auth| A2
    C -.->|3ï¸âƒ£ Direct gRPC Stream<br/>with JWT Auth| A3
    
    %% Agent Management
    A1 -.->|ğŸ’“ Health & Registration| CO
    A2 -.->|ğŸ’“ Health & Registration| CO
    A3 -.->|ğŸ’“ Health & Registration| CO
    
    %% Model Management
    A1 --> M1
    A2 --> M2
    A3 --> M3
    
    %% Styling
    classDef clientStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef coordStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef agentStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef modelStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef engineStyle fill:#fce4ec,stroke:#c2185b,stroke-width:1px,color:#000
    
    class C clientStyle
    class CO coordStyle
    class A1,A2,A3 agentStyle
    class M1,M2,M3 modelStyle
    class MCDA,JWT engineStyle
```

## ğŸ¯ Use Cases

### **High-Throughput Inference Workloads**
- Multiple concurrent LLM requests requiring optimal resource utilization
- Real-time applications needing low latency and high availability
- Production systems requiring automatic failover and load balancing

### **Multi-GPU Deployments**
- Distributed inference across multiple compute nodes
- Intelligent quantization for memory-constrained environments
- GPU resource optimization with VRAM-aware scheduling

### **Development and Testing**
- Local development with multiple model variants
- A/B testing different quantization strategies
- Performance benchmarking and optimization

### **Edge Computing**
- Resource-constrained edge deployments
- Automatic quantization based on available hardware
- Efficient model distribution and caching

## ğŸš€ Quick Start

```bash
# Deploy the system
./scripts/deploy.sh

# Test with REST API
cd client && python rest_client.py

# View system status
./scripts/deploy.sh status
```

For detailed setup instructions, see [QUICKSTART.md](QUICKSTART.md).

## ğŸ“ Project Structure

```
TitanCompute/
â”œâ”€â”€ coordinator/          # Go-based coordinator service
â”œâ”€â”€ agent/               # Python-based agent service  
â”œâ”€â”€ proto/               # gRPC protocol definitions
â”œâ”€â”€ client/              # Client examples and testing tools
â”œâ”€â”€ scripts/             # Deployment and validation scripts
â”œâ”€â”€ docker-compose.yml   # Container orchestration
â”œâ”€â”€ QUICKSTART.md       # Detailed setup guide
â””â”€â”€ README.md           # This file
```

## ğŸ”„ Request Flow

1. **Client** â†’ `POST /api/v1/inference/request` â†’ **Coordinator**
2. **Coordinator** â†’ MCDA scheduling â†’ Select optimal agent â†’ Generate JWT token
3. **Client** â† JWT token + agent endpoint â† **Coordinator**  
4. **Client** â†’ Direct gRPC streaming with JWT â†’ **Selected Agent**
5. **Agent** â†’ JWT validation â†’ Model inference â†’ Stream results â†’ **Client**

## ğŸ“Š System Endpoints

| Service | gRPC | Description | M2 Features |
|---------|------|-------------|-------------|
| Coordinator | :50051 | MCDA routing + JWT auth | Circuit breaker, memory-aware scheduling |
| Agent 1 | :50052 | Direct streaming | GGUF quantization, JWT validation |
| Agent 2 | :50053 | Direct streaming | GGUF quantization, JWT validation |

## ğŸ§ª Testing

All testing utilities are located in the [`client/`](client/) directory:

```bash
# REST API testing (recommended)
cd client && python rest_client.py

# Shell/curl testing  
cd client && ./test_curl.sh
```

For detailed testing options, see [`client/README.md`](client/README.md).

## âš™ï¸ Configuration

### Environment Variables

**Coordinator (M2 Enhanced):**
- `COORDINATOR_PORT=50051`: gRPC port
- `HEARTBEAT_TIMEOUT=30s`: Agent health timeout
- `TOKEN_TTL=300s`: JWT token lifetime (5 minutes)
- `CLEANUP_INTERVAL=60s`: Registry cleanup interval

**Agent (M2 Enhanced):**
- `AGENT_ID=agent-1`: Unique identifier
- `COORDINATOR_ENDPOINT=coordinator:50051`: Coordinator address
- `PUBLIC_HOST=localhost`: Public hostname for clients
- `MAX_CONCURRENT_JOBS=4`: Job capacity
- `SUPPORTED_MODELS=llama3.1:8b-instruct-q4_k_m`: Default models
- `OLLAMA_HOST=http://localhost:11434`: Ollama service endpoint

### GGUF Quantization Configuration
The system automatically selects quantization based on available memory:

| Memory Tier | Quantizations | Description |
|-------------|---------------|-------------|
| Premium (8GB+) | Q8_0, Q6_K_L, Q6_K | Near-original quality |
| High (6-8GB) | Q5_K_M, Q4_K_M, Q4_K_S | Balanced quality/size |
| Good (4-6GB) | IQ4_XS, Q3_K_L, IQ3_M | Efficient compression |
| Emergency (<4GB) | Q2_K, IQ2_M | Minimal memory usage |

## ğŸ”§ Operations

### Service Management
```bash
# Start all services
./scripts/deploy.sh

# Stop services
./scripts/deploy.sh stop

# View system status
./scripts/deploy.sh status
```

### Model Management
```bash
# Access agent container
docker exec -it titan-agent-1 bash

# List available models
ollama list

# Pull models (system auto-selects quantization)
ollama pull llama3.1:8b-instruct-q4_k_m
```

### System Monitoring
```bash
# Monitor system resources
docker stats titan-coordinator titan-agent-1 titan-agent-2

# Check logs
docker-compose logs coordinator
docker-compose logs agent-1
```

### Quick API Testing
```bash
# REST API (direct inference)
curl -X POST http://localhost:8080/api/v1/inference/request \
  -H "Content-Type: application/json" \
  -d '{"client_id": "test", "model": "llama3.1:8b-instruct-q4_k_m", "prompt": "Hello!"}'

# Health check
curl http://localhost:8080/health
```

### Troubleshooting
```bash
# Check service health
curl http://localhost:8080/health

# View detailed logs
docker-compose logs -f
```

## ğŸ“Š Performance Targets

| Metric | Target | Implementation |
|--------|--------|----------------|
| Time-to-First-Token | < 500ms (p95) | MCDA scheduling + model preloading |
| Inter-Token Latency | < 50ms | Direct streaming + GGUF optimization |
| Concurrent Streams/Agent | 4-8 streams | GPU memory management |
| System Availability | > 99.5% | Circuit breaker fault tolerance |
| Memory Efficiency | 30-60% reduction | GGUF quantization |

## ğŸ¯ Production Features

### Memory-Aware MCDA Scheduling
- Multi-Criteria Algorithm: 40% VRAM + 30% Jobs + 20% RTT + 10% Performance
- Intelligent model placement with automatic VRAM estimation
- Real-time adaptation based on current system state

### Circuit Breaker Fault Tolerance
- Three-state pattern: Closed â†’ Open â†’ Half-Open
- Automatic recovery with configurable thresholds
- Graceful degradation for struggling agents

### GGUF Quantization Support
- Full Bartowski range: Q8_0 to IQ2_M (13 formats)
- Memory-aware selection based on available RAM
- Intelligent fallback for memory-constrained environments

### JWT Authentication & Security
- RSA-256 signed tokens with 5-minute TTL
- Secure agent validation via public key distribution
- Industry-standard cryptographic security

## ï¿½ Documentation

For detailed component documentation, see:
- [`agent/README.md`](agent/README.md) - Python agent implementation
- [`coordinator/README.md`](coordinator/README.md) - Go coordinator service  
- [`proto/README.md`](proto/README.md) - Protocol buffer definitions
- [`client/README.md`](client/README.md) - Client examples and testing
- [`scripts/README.md`](scripts/README.md) - Deployment and validation scripts

---

**TitanCompute** - Production-ready distributed LLM inference with zero-proxy streaming architecture.

### ğŸ”„ Zero-Proxy Architecture Advantage

```mermaid
flowchart LR
    subgraph "Traditional Proxy Architecture"
        direction LR
        C1[ğŸ‘¤ Client] --> P[ğŸ”„ Proxy<br/>Bottleneck] --> A1T[ğŸ¤– Agent]
        P --> A2T[ğŸ¤– Agent]
        P --> A3T[ğŸ¤– Agent]
    end
    
    subgraph "TitanCompute Zero-Proxy"
        direction LR
        C2[ğŸ‘¤ Client] --> CO2[ğŸ§  Coordinator<br/>Selection Only]
        CO2 -.->|JWT + Endpoint| C2
        C2 -.->|Direct Stream| A1Z[ğŸ¤– Agent]
        C2 -.->|Direct Stream| A2Z[ğŸ¤– Agent] 
        C2 -.->|Direct Stream| A3Z[ğŸ¤– Agent]
    end
    
    classDef bottleneck fill:#ffcdd2,stroke:#d32f2f,stroke-width:3px
    classDef efficient fill:#c8e6c9,stroke:#388e3c,stroke-width:2px
    classDef client fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class P bottleneck
    class CO2,A1Z,A2Z,A3Z efficient
    class C1,C2 client
```
